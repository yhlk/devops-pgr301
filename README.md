## Eksamen devops 2024

# Oppgave 1: Generere og deploye en serverless-applikasjon med SAM

## üõ†Ô∏è M√•l
Vi opprettet en serverless-applikasjon som genererer bilder basert p√• tekstprompts ved hjelp av **AWS Lambda** og **AWS SAM**.

---

## üöÄ Trinn-for-trinn-guide

### 1. Opprettelse av prosjekt
For √• starte et nytt prosjekt, brukte vi `sam init`:

sam init

Valg under init-prosessen:

   - Template: AWS Quick Start Templates
   - Runtime: Python 3.8
   - Application name: sam-app-tael002

2. Implementasjon av Lambda-funksjonen

Lambda-funksjonen ble implementert i application.py. Den tar imot tekstprompts og genererer bilder ved hjelp av Bedrock-modellen. Genererte bilder lastes opp til en S3-bucketen vi har tilgjengelig fra l√¶rer.

3. Bygging av applikasjonen

For √• bygge applikasjonen kj√∏rte vi:

sam build

4. Deploy av applikasjonen

Deploy ble gjennomf√∏rt ved hjelp av sam deploy --guided:

sam deploy --guided

Under deploy spesifiserte vi n√∏dvendige parametere som:

    BucketName: pgr301-couch-explorers
    KandidatNr: 18
    region: eu-west-1

5. Testing av applikasjonen

Etter deploy sendte vi POST-foresp√∏rsler til API Gateway for √• teste funksjonaliteten:

curl -X POST https://opypl77il0.execute-api.eu-west-1.amazonaws.com/Prod/generate \
-H "Content-Type: application/json" \
-d '{"prompt": "A croissant"}'




# Oppgave 2: Infrastruktur med Terraform og SQS
Kommandoer vi brukte

    terraform init
    Initialiserte Terraform-prosjektet og konfigurerte backend for lagring av state-fil i S3-bucketen pgr301-2024-terraform-state.

    terraform validate
    Validerte at Terraform-koden var syntaktisk korrekt og klar for bruk.

    terraform plan
    Genererte en plan for infrastrukturen som viste hvilke ressurser som ville bli opprettet, endret eller slettet.

    terraform apply
    Implementerte endringene ved √• opprette, endre eller fjerne ressurser i AWS basert p√• Terraform-koden.

A. Infrastruktur som kode

For √• l√∏se ytelsesproblemene brukte vi Amazon SQS som mellomledd mellom klientene og bildeprosesseringskoden. Dette sikret en mer skalerbar l√∏sning. Vi utf√∏rte f√∏lgende steg:

    Terraform-konfigurasjon for Lambda og SQS:
        Opprettet en Lambda-funksjon som behandler meldinger fra SQS-k√∏en.
        Konfigurerte en SQS-k√∏ som mellomledd mellom klient og bildebehandlingsfunksjonen.
        Integrerte Lambda-funksjonen med SQS for asynkron behandling.
        Opprettet n√∏dvendige IAM-roller og policyer for Lambda-funksjonen:
            Lesetilgang til SQS.
            Skrivetilgang til S3-bucketen pgr301-couch-explorers.
        Lagret Terraform state-filen i S3-bucketen pgr301-2024-terraform-state.

    Lagring i S3:
        Lambda-funksjonen ble konfigurert til √• laste opp genererte bilder til S3-bucketen pgr301-couch-explorers/images.

    Timeout-konfigurasjon:
        Justerte Lambda-funksjonens timeout til 10 sekunder for √• h√•ndtere lengre prosesseringstider.

B. GitHub Actions Workflow for Terraform

For √• automatisere deploy av infrastrukturen brukte vi GitHub Actions. Workflowen ble satt opp med f√∏lgende regler:

    Hovedbranch (main):
        Workflow kj√∏rer terraform apply for √• oppdatere infrastrukturen automatisk med endringene.

    Feature branches (testing-terraform):
        Workflow kj√∏rer terraform plan for √• vise en plan for infrastrukturendringer uten √• implementere dem.

Leveranser

    Lenker til workflow-kj√∏ringer:
        Workflow-kj√∏ring med terraform apply p√• main: https://github.com/yhlk/devops-pgr301/actions/runs/12016974033
        Workflow-kj√∏ring med terraform plan p√• andre brancher: https://github.com/yhlk/devops-pgr301/actions/runs/11988700165
    SQS URL: URL-en til den opprettede SQS-k√∏en for testing av meldinger: https://sqs.eu-west-1.amazonaws.com/244530008913/taelqueue

# Oppgave 3: Javaklient og Docker
A. Docker-image for SQS-klienten

For √• forenkle bruken av SQS-klienten uten behov for √• installere Java lokalt, opprettet vi et Docker-image. Her er prosessen vi fulgte:

    Pull av Docker-imaget
    Teamet kan enkelt hente imaget fra Docker Hub ved √• bruke f√∏lgende kommando:

docker pull tael002/java-sqs-client-tael002:latest

Opprettelse av Dockerfile
Vi brukte en multi-stage build for √• optimalisere Docker-imaget:

    F√∏rste stage:
        Kompilerte Java-koden ved hjelp av Maven.
        Bygde en JAR-fil fra koden.
    Andre stage:
        Kopierte n√∏dvendig output fra f√∏rste stage.
        Kj√∏rte applikasjonen i et lettvekts runtime-milj√∏ som openjdk:17-jre-slim.

Testing av Docker-imaget
Vi testet Docker-imaget ved √• sende meldinger til SQS-k√∏en.
Kommando for testing:

    docker run \
        -e AWS_ACCESS_KEY_ID=AKIATR3Y72NI5E2TSO7R \
        -e AWS_SECRET_ACCESS_KEY=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 \
        -e SQS_QUEUE_URL=https://sqs.eu-west-1.amazonaws.com/244530008913/taelqueue \
        tael002/java-sqs-client-tael002 "Me on top of a pyramid"

B. GitHub Actions Workflow for Docker

For √• automatisere bygging og publisering av Docker-imaget opprettet vi en GitHub Actions workflow. Denne sikret at teamet alltid hadde tilgang til oppdatert funksjonalitet.

    Trigger for workflowen
        Workflow kj√∏rer automatisk ved hver push til main.
        Endringer i main resulterer i en oppdatert versjon av Docker-imaget.

    Publisering til Docker Hub
        Vi konfigurerte en Docker Hub-konto for √• motta publiserte images.
        Workflow logger inn p√• Docker Hub, bygger imaget, og pusher det automatisk.

    Tagging-strategi
        Latest tag:
        Imaget tagges alltid med latest for enkel tilgang til den nyeste versjonen.
        Versjonskontroll:
        Imaget tagges ogs√• med spesifikke versjonsnummer (f.eks. 1.0.0) for historikk og kontroll.

Leveranser

    Docker Hub Image:

docker pull tael002/java-sqs-client-tael002:latest

Kommando for testing:
For √• teste funksjonaliteten, kj√∏r f√∏lgende kommando:

// vet at man ikke skal dele access key og secret key, men viser det som eksempel her kun for sensor
    docker run \
        -e AWS_ACCESS_KEY_ID=AKIATR3Y72NI5E2TSO7R \
        -e AWS_SECRET_ACCESS_KEY=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 \
        -e SQS_QUEUE_URL=https://sqs.eu-west-1.amazonaws.com/244530008913/taelqueue \
        tael002/java-sqs-client-tael002 "Me on top of a pyramid"

Denne prosessen sikret enkel bruk og rask distribusjon av klienten

# Oppgave 4: Metrics og overv√•kning
A. CloudWatch-alarm for OldestMessageAlarm_TaelQueue

For √• overv√•ke l√∏sningen og sikre rask tilbakemelding, satte jeg opp en CloudWatch-alarm basert p√• SQS-metrikken ApproximateAgeOfOldestMessage.

    Terraform-kode:
        Alarmen trigges n√•r verdien overstiger en angitt terskel i mitt tilfelle 120.
        E-postvarsling er satt opp via Amazon SNS. E-postadressen spesifiseres som en variabel i Terraform-koden. Brukte student-eposten.








# Oppgave 5: Serverless, Function as a Service vs Container-teknologi

Denne dr√∏ftelsen utforsker implikasjonene av √• velge mellom en **serverless arkitektur** med Function-as-a-Service (FaaS), som AWS Lambda, og en **mikrotjenestearkitektur**, sett opp mot sentrale DevOps-prinsipper.

---

## 1. Automatisering og Kontinuerlig Levering (CI/CD)

### Serverless Arkitektur
**Styrker:**
- FaaS-tjenester som AWS Lambda er tettere integrert med CI/CD-verkt√∏y som AWS CodePipeline, noe som muliggj√∏r enklere automatisering av utrullinger.
- Hver funksjon kan ha sin egen utrullingsprosess, noe som gir mulighet for raskere endringer i spesifikke deler av systemet uten √• p√•virke andre komponenter.
- "Zero-downtime deploys" blir enklere ved hjelp av versjonskontroll og trafikksplitting (f.eks. AWS Lambda Alias og Canary Deployments).

**Svakheter:**
- Flere funksjoner f√∏rer til flere pipelines, noe som kan √∏ke kompleksiteten i administrasjonen av CI/CD-prosesser.
- Testing av serverless-funksjoner krever ofte mock-tjenester eller milj√∏er som simulerer AWS-integrasjoner, noe som kan v√¶re utfordrende.

### Mikrotjenestearkitektur
**Styrker:**
- CI/CD-pipelines for mikrotjenester er mer etablerte og st√∏ttet av mange verkt√∏y som Jenkins, GitHub Actions og ArgoCD.
- En samlet containerbasert pipeline kan redusere kompleksiteten sammenlignet med serverless-funksjoner som krever flere separate pipelines.

**Svakheter:**
- Rulling ut endringer kan kreve koordinasjon mellom tjenester, spesielt hvis de deler ressurser som databaser.
- Oppdatering av containere krever mer arbeid, som √• bygge, teste og publisere Docker-images.

---

## 2. Observability (Overv√•kning)

### Serverless Arkitektur
**Styrker:**
- AWS tilbyr integrasjoner som CloudWatch og X-Ray for logging, sporing og feils√∏king.
- Funksjonsbasert isolasjon gj√∏r det lettere √• spore spesifikke feil tilbake til individuelle komponenter.

**Svakheter:**
- Logging og overv√•kning blir ofte fragmentert. Hver funksjon m√• ha tilstrekkelig logging for √• kunne spore gjennom hele flyten.
- "Cold start"-problemer kan v√¶re vanskelige √• oppdage og overv√•ke.
- Distribuert natur skaper utfordringer med sammenhengende tracing p√• tvers av funksjoner og tjenester.

### Mikrotjenestearkitektur
**Styrker:**
- Observability-verkt√∏y som Prometheus, Grafana og Jaeger gir omfattende overv√•kning og tracing p√• tvers av tjenester.
- Feils√∏king kan v√¶re enklere i systemer med sammenhengende tjenester som kj√∏rer i container-clustere (f.eks. Kubernetes).

**Svakheter:**
- Mer kompleks logging n√•r tjenester kommuniserer via mange ulike nettverksprotokoller.
- Overv√•kning av containere krever oppsett og vedlikehold av egne observability-l√∏sninger, noe som kan kreve mer arbeid enn serverless.

---

## 3. Skalerbarhet og Kostnadskontroll

### Serverless Arkitektur
**Styrker:**
- Automatisk skalerbarhet er innebygd. Lambda skalerer basert p√• antall foresp√∏rsler uten behov for manuell intervensjon.
- "Pay-as-you-go"-modell gir direkte kostnadsoptimalisering. Du betaler kun for tiden funksjonen kj√∏rer.

**Svakheter:**
- H√∏ye trafikkvolumer kan f√∏re til uventet h√∏ye kostnader, spesielt for tjenester som kalles ofte.
- Begrensninger i samtidige foresp√∏rsler (f.eks. "concurrent execution limits") kan f√∏re til flaskehalsproblemer.

### Mikrotjenestearkitektur
**Styrker:**
- Kostnadene kan forutses mer n√∏yaktig siden containerbaserte l√∏sninger vanligvis krever faste ressurser som EC2-instanser.
- Skalerbarhet kan kontrolleres mer presist ved √• tilpasse ressurser i Kubernetes eller ECS.

**Svakheter:**
- Over- eller underprovisjonering kan f√∏re til ineffektiv ressursbruk.
- Manuell skalering og vedlikehold av infrastrukturen krever mer DevOps-innsats.

---

## 4. Eierskap og Ansvar

### Serverless Arkitektur
**Styrker:**
- Infrastrukturansvar flyttes til AWS. DevOps-teamet kan fokusere p√• applikasjonslogikk fremfor infrastruktur.
- Serverless-tjenester er enklere √• administrere med mindre vedlikehold sammenlignet med containere.

**Svakheter:**
- Manglende kontroll over infrastrukturen kan f√∏re til vanskeligheter med feils√∏king, spesielt i komplekse systemer.
- DevOps-team m√• holde oversikt over flere sm√• funksjoner, noe som kan f√∏re til "function sprawl".

### Mikrotjenestearkitektur
**Styrker:**
- Mer kontroll over infrastrukturen gir mulighet for tilpasning, noe som er viktig i komplekse systemer.
- Hver mikrotjeneste har et dedikert team, noe som styrker eierskapet.

**Svakheter:**
- Mer ansvar for administrasjon av infrastruktur som containere, orkestrering og oppsett av milj√∏er.
- Overv√•king av mange separate tjenester kan v√¶re utfordrende.

---

## Konklusjon

### N√•r velge **Serverless**:
- Uforutsigbar eller varierende trafikk.
- Fokus p√• rask utvikling og minimal infrastrukturadministrasjon.
- Kostnadseffektivitet for mindre arbeidsmengder.

### N√•r velge **Mikrotjenester**:
- Komplekse avhengigheter eller konsistent ytelse er n√∏dvendig.
- Full kontroll over infrastruktur og ressursutnyttelse.
- Behov for √• integrere med eldre systemer eller spesifikke containermilj√∏er.

**Valget mellom de to arkitekturene b√∏r tilpasses organisasjonens behov, ferdigheter og forretningsm√•l. Ofte kan en hybridl√∏sning v√¶re optimal.**
